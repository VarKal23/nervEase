# nervEase
Ease your nerves with nervEase!

A public speaking simulator to help users overcome fear and anxiety

<img width="254" alt="logo3" src="https://user-images.githubusercontent.com/61900832/167345651-7d793152-257b-4558-adab-2d0bdf8e64c7.png">

Demo video: https://drive.google.com/file/d/1EteGJ5gIrYJ6F6IXRNto7hheZ4KtnhgA/view?usp=sharing

Tech:
- Tushar Kohli
- Anish Mankal
- Varun Kalappa
- Meghana Inaganti
- Kevin Li

Business:
- Anchit Nath
- Mughda Konda

Product:
- Faleha Quadri

Leads:
- Connor Poff

## Project Status
This project was developed as part of Texas Convergent's build teams which is a semester-long competition. While there are still a lot of features that could be added, given the time constraints, the features implemented are the ones that were presented. There are no further plans to continue building this project.

Key Features:
- Custom public speaking VR environment
- Heart rate and words per minute tracking metrics
- Compilation and processing of metrics and display through application

# Demo Video

## Installation and Setup Instructions
A demo video has been included as installation and setup of this project is complex at the moment requiring Swift/XCode build, Google Cardboard, iPhone, and Apple Watch. Additionally, a connecting piece of code is missing.

## Reflection
This project as previously mentioned was a part of the Texas Convergent Build Teams program and my team came up with this application idea because we felt that current resources to help with public speaking weren't effective enough and not as accessible.

This project was challenging because prior to this, we had never worked with virtual reality and knew little to nothing about it. Through lots of research and trial and error, we were able to put out an actual VR simulation that can help users practice their communication skills.

The technologies we used were Unity, Swift + XCode, iPhone, Apple Watch, BE HealthKit, and Google Cardboard. We used Unity to actually build the VR environment which was actually based on the UT computer science lecture hall in the Gates Dell Complex. The Apple watch was used to gather the heart rate data and the iPhone was used for its microphone to capture the words per minute metrics. We used the Apple Health Kit API to gather and use all this information for our applications use. Swift and XCode were used to create an ios application that bundled all the different parts of our project into one program launchable from an iPhone. Once the app is launched, the Google Cardboard headset was used with the iPhone to actually use the application.

